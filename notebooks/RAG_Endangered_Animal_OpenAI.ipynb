{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "from pymongo import MongoClient\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the openai.api_key to the API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "def get_database():\n",
    "    \"\"\"\n",
    "    Purpose: Get the database from MongoDB\n",
    "    \"\"\"\n",
    "    uri = \"mongodb+srv://sriveerisetti:SuperAnimal@saveanimal.caz0ya1.mongodb.net/?retryWrites=true&w=majority&appName=SaveAnimal\"\n",
    "    ca = certifi.where()\n",
    "    client = MongoClient(uri, tlsCAFile=ca)\n",
    "    db = client['OpenSaveAnimal']  # Make sure to use the correct database name\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_words(text, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Purpose: Split the text into chunks of the specified size.\n",
    "    Input: text - the text to be split\n",
    "    Input: chunk_size - the size of the chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    # Iterate over the words and yield chunks of the specified size\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield ' '.join(words[i:i+chunk_size])\n",
    "\n",
    "def generate_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Purpose: Generate an embedding for the specified text using the openai.Embedding API.\n",
    "    Input: text - the text for which to generate the embedding\n",
    "    Input: model - the model to use for generating the embedding\n",
    "    \"\"\"\n",
    "    response = openai.Embedding.create(\n",
    "        input=[text],  #\n",
    "        model=model\n",
    "    )\n",
    "    embedding = response['data'][0]['embedding']\n",
    "    return embedding\n",
    "\n",
    "def store_text_with_embedding(text, source, collection):\n",
    "    \"\"\"\n",
    "    Purpose: Store the text and its embedding in the database.\n",
    "    Input: text - the text to store\n",
    "    Input: source - the source of the text\n",
    "    Input: collection - the collection in which to store the text\n",
    "    \"\"\"\n",
    "    for chunk in chunk_words(text):\n",
    "        chunk_embedding = generate_embedding(chunk)\n",
    "        collection.insert_one({\n",
    "            \"chunk\": chunk,\n",
    "            \"embedding\": chunk_embedding,\n",
    "            \"source\": source\n",
    "        })\n",
    "    print(f\"Content from {source} has been successfully stored in MongoDB.\")\n",
    "\n",
    "def process_text_files(folder_path, collection):\n",
    "    \"\"\"\n",
    "    Purpose: Process all the text files in the specified folder and store the text and its embedding in the database.\n",
    "    Input: folder_path - the path to the folder containing the text files\n",
    "    Input: collection - the collection in which to store the text\n",
    "    \"\"\"\n",
    "    # The for loop goes through each file in the folder and processes the text files\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Capture all the text files in the folder\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Here we open the txt file and read the content\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text_content = file.read()\n",
    "                # Here we use the store_text_with_embedding function to store the text and its embedding in the database\n",
    "                store_text_with_embedding(text_content, filename, collection)\n",
    "\n",
    "# The main function calls the process_text_files function to process the text files and store the text and its embedding in the database\n",
    "if __name__ == \"__main__\":\n",
    "    db = get_database()\n",
    "    collection = db['SaveAnimal']  \n",
    "    folder_path = \"/Users/sveerisetti/Desktop/Duke_Spring/Deep_Learning/Projects/Invidual_Project/Notebooks/Animal_Information\"\n",
    "    process_text_files(folder_path, collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Purpose: Find the most relevant chunks for the specified query using the cosine similarity.\n",
    "    Input: query - the user's query\n",
    "    Input: top_k - the number of most relevant chunks to return\n",
    "    \"\"\"\n",
    "    db = get_database()\n",
    "    collection = db['SaveAnimal']\n",
    "    # Here we can use the generate_embedding function to generate an embedding for the query\n",
    "    query_embedding = np.array(generate_embedding(query)).reshape(1, -1) \n",
    "    docs = collection.find({})\n",
    "\n",
    "    similarities = []\n",
    "    for doc in docs:\n",
    "        chunk_embedding = np.array(doc['embedding']).reshape(1, -1)  \n",
    "        # We can use the cosine similarity to find the similarity between the query and the chunk\n",
    "        similarity = cosine_similarity(chunk_embedding, query_embedding)[0][0]\n",
    "        similarities.append((doc['chunk'], similarity, doc.get('source')))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    seen_chunks = set()\n",
    "    unique_similarities = []\n",
    "    # We create a for loop that goes through the similarities and stores the unique chunks\n",
    "    for chunk, similarity, source in similarities:\n",
    "        if chunk not in seen_chunks:\n",
    "            seen_chunks.add(chunk)\n",
    "            # For all unique chunks, we store the chunk, similarity, and source\n",
    "            unique_similarities.append((chunk, similarity, source))\n",
    "            if len(unique_similarities) == top_k:\n",
    "                break\n",
    "    return unique_similarities\n",
    "\n",
    "def generate_prompt_with_context(relevant_chunks, query):\n",
    "    \"\"\"\n",
    "    Purpose: Generate a prompt with the context of the most relevant chunks and the user's query.\n",
    "    Input: relevant_chunks - the most relevant chunks\n",
    "    Input: query - the user's query\n",
    "    \"\"\"\n",
    "    # Here, we are creating a prompt that includes the context of the most relevant chunks and the user's query\n",
    "    context = \"Based on the following information: \"\n",
    "    # Here we iterate over the most relevant chunks and add them to the context\n",
    "    for chunk, similarity, source in relevant_chunks:\n",
    "        context += f\"\\n- [Source: {source}]: {chunk}\"\n",
    "    prompt = f\"{context}\\n\\n{query}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_text_with_gpt35(prompt, max_tokens=3100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Purpose: Generate text using the GPT-3.5 model with adjustable randomness.\n",
    "    Input: prompt - the prompt for the model\n",
    "    Input: max_tokens - the maximum number of tokens to generate\n",
    "    Input: temperature - controls the randomness of the output, higher values lead to more varied outputs\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert on endangered species.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        # To increase randomness, we can adjust the temperature parameter\n",
    "        temperature=temperature,  \n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def get_response_for_query(query, temperature=0.9):\n",
    "    \"\"\"\n",
    "    Purpose: Get a response for the specified query, allowing temperature adjustment for output variability.\n",
    "    Input: query - the user's query\n",
    "    Input: temperature - controls the randomness of the output, higher values lead to more varied outputs\n",
    "    \"\"\"\n",
    "    # We can use the find_most_relevant_chunks function to find the most relevant chunks for the query\n",
    "    relevant_chunks = find_most_relevant_chunks(query)\n",
    "    if relevant_chunks:\n",
    "        # Here we use the generate_prompt_with_context function to generate a prompt with the context of the most relevant chunks\n",
    "        prompt = generate_prompt_with_context(relevant_chunks, query)\n",
    "    else:\n",
    "        prompt = query  \n",
    "    # Here we use the generate_text_with_gpt35 function to generate text using the GPT-3.5 model\n",
    "    return generate_text_with_gpt35(prompt, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_response_for_query(\n",
    "\"The orangutan is a broad category that encompasses several distinct species, each with its unique adaptations, habitats, and challenges. Provide an overview of the diversity within the orangutan species, detailing the subspecies known, including their physical characteristics, geographical distribution, and the conservation challenges, and endangered status. Highlight the differences and similarities among these subspecies to give a comprehensive understanding of the species' ecological and conservation status. Please also provide how the WWF proposes to help the orangutan species and the conservation efforts in place to protect the orangutan species.\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orangutans are a fascinating and critically endangered species of great apes found in the rainforests of Borneo and Sumatra. There are three distinct species of orangutans, each with its unique physical characteristics, geographical distribution, and conservation challenges.\n",
      "\n",
      "1. Bornean Orangutans:\n",
      "- There are three subspecies of Bornean orangutans: Northwest, Northeast, and Central.\n",
      "- Northwest Bornean orangutans are the most threatened subspecies, with only around 1,500 individuals remaining. Their habitat has been severely affected by logging and hunting.\n",
      "- Northeast Bornean orangutans are the smallest in size and are found in Sabah and eastern Kalimantan.\n",
      "- Central Bornean orangutans have the highest population, with at least 35,000 individuals.\n",
      "- Conservation challenges for Bornean orangutans include habitat loss due to logging, hunting, and conversion of forests to agriculture.\n",
      "\n",
      "2. Sumatran Orangutans:\n",
      "- The range of Sumatran orangutans is restricted to the northern part of Sumatra.\n",
      "- There are nine existing populations, with only seven having prospects of long-term viability.\n",
      "- The species is critically endangered, with an estimated total population of about 7,500 individuals.\n",
      "- Conservation efforts include reintroduction programs for confiscated or pet orangutans and protection of key habitats.\n",
      "\n",
      "3. Tapanuli Orangutans:\n",
      "- The most recently discovered species of orangutan is the Tapanuli orangutan, with less than 800 individuals remaining.\n",
      "- This species is the most endangered of all great apes.\n",
      "- The species faces threats from habitat loss, hunting, and other human activities.\n",
      "\n",
      "Similarities among the orangutan species include their arboreal nature, red fur, and shared genetic makeup with humans. They are important seed dispersers in their ecosystems and play a crucial role in maintaining forest health.\n",
      "\n",
      "The WWF has proposed several conservation efforts to help orangutan species, including:\n",
      "- Habitat protection and restoration to ensure the survival of orangutans in the wild.\n",
      "- Working with local communities to promote sustainable resource management practices.\n",
      "- Supporting anti-poaching efforts to reduce illegal hunting of orangutans.\n",
      "- Monitoring orangutan populations and conducting research to better understand their behavior and conservation needs.\n",
      "\n",
      "Overall, orangutans face severe threats due to habitat loss, hunting, and illegal trade. Conservation efforts are crucial to ensuring the survival of these amazing species and the protection of their unique ecosystems.\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
